{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a87bdc59-7424-4892-b455-1a896c1baf09",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a46d2-66c0-4b26-8791-4716f6b9d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a62d23-73eb-42e2-808f-339e2d5cae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "print()\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(\"Memory Usage:\")\n",
    "    print(\"Allocated:\", round(torch.cuda.memory_allocated(0)/1024**3,1), \"GB\")\n",
    "    print(\"Cached:   \", round(torch.cuda.memory_reserved(0)/1024**3,1), \"GB\")\n",
    "\n",
    "torch.set_default_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd9b1e4-781b-45f0-b93d-81dffdaec0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050d0024-00f5-4176-a8d7-c63281f4b525",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44417929-4273-47dd-ab10-4f47046b3bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
    "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
    "# of shape (2000, 3) \n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. The Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
    "# to match the shape of `y`.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + ' + \\\n",
    "      f'{linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4702d8c4-ee13-41e2-9039-dbfb7675fa2a",
   "metadata": {},
   "source": [
    "## Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c491b2b4-dad8-44f7-a832-42c8e7e44f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/en/llm_tutorial\n",
    "# https://huggingface.co/docs/transformers/en/tasks/prompting\n",
    "# https://huggingface.co/docs/transformers/en/quantization\n",
    "# https://huggingface.co/docs/accelerate/usage_guides/big_modeling\n",
    "# https://huggingface.co/docs/hub/en/sentence-transformers\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "llm_model_name = \"openai-community/gpt2\"\n",
    "# llm_model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "# llm_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "embed_model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "# embed_model_name\"BAAI/bge-large-en-v1.5\"\n",
    "embed_model_name = \"BAAI/llm-embedder\"\n",
    "\n",
    "reranker_model_name = \"BAAI/bge-reranker-base\"\n",
    "# reranker_model_name = \"BAAI/bge-reranker-large\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(llm_model_name)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0045e3c3-4511-494c-addf-ecd4c4960d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = SentenceTransformer(embed_model_name)\n",
    "sentence = [\"This framework generates embeddings for each input sentence\"]\n",
    "embedding = embed_model.encode(sentence)\n",
    "len(embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5ba082-56f6-44bc-8d67-d74b860890dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_model_name,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    llm_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    padding_side=\"left\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer([\"A list of colors: red, blue\"], padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = llm_model.generate(**inputs, max_new_tokens=20, pad_token_id=50256, do_sample=True)\n",
    "text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5343bf24-5c3d-4ff8-bf7f-19a4c03729cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name)\n",
    "model.eval()\n",
    "\n",
    "pairs = [[\"what is panda?\", \"hi\"], [\"what is panda?\", \"The giant panda is a bear species endemic to China.\"]]\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6abee66-f9dd-43b1-9afb-784b85bb4c33",
   "metadata": {},
   "source": [
    "## Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1589317-fa43-4d44-89e6-ff1162564f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4882f8b-1d85-4617-be9b-2dd56154c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embed = HuggingFaceBgeEmbeddings(\n",
    "    model_name=embed_model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "embedding = embed.embed_query(\"This framework generates embeddings for each input sentence\")\n",
    "len(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc10e453-8538-4dc8-ab73-6b91bb4af7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_name = \"openai-community/gpt2\"\n",
    "pipeline_kwargs = {\"max_new_tokens\": 20, \"pad_token_id\": 50256, \"do_sample\": True}\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=llm_model_name,\n",
    "    task=\"text-generation\",\n",
    "    device_map=\"auto\",\n",
    "    pipeline_kwargs=pipeline_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcabbc2-39b5-465f-bdcd-244937f166bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "chain = prompt | llm\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2fc3ce-a6b5-4ff8-8cde-e3a73728927e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 - Deep Learning",
   "language": "python",
   "name": "deep_learning_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
